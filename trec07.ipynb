{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I have used TREC 2007 Public Corpus [availiable online](http://plg.uwaterloo.ca/~gvcormac/treccorpus07/). My purpose is to classify e-mails to spam and not spam e-mails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##First\n",
    "First of all, I upload TREC07 corpus and extract it somewhere. After that i read index file that includes a name of a file and its label ('class'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many e-mails are availiable? - 75419 e-mails\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "target = pd.read_csv('./trec07Data/data/index', sep=' ', names = ['class', 'file'])\n",
    "print(\"How many e-mails are availiable? - %i e-mails\"%len(target))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many spam e-mails?  - 50199 \nHow many non-spam e-mails?  - 25220 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def printSpamHamStat():\n",
    "    spam = np.where((target['class']=='spam')==True)[0].size\n",
    "    Spam = np.where((target['class']=='Spam')==True)[0].size\n",
    "\n",
    "    ham = np.where((target['class']=='ham')==True)[0].size\n",
    "    Ham = np.where((target['class']=='Ham')==True)[0].size\n",
    "\n",
    "    print(\"How many spam e-mails?  - %i \"% (spam+Spam))\n",
    "    print(\"How many non-spam e-mails?  - %i \"% (ham+Ham))\n",
    "    \n",
    "printSpamHamStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Second\n",
    "\n",
    "I'm creating an e-mail dataframe while reading all the files from the specified folder that are included in the index file. The email content is stored in __emails__ while __UnicodeDecodeError__ is catched and indices of problematic e-mails are stored to avoid from __target__ for the futher consideration. \n",
    "\n",
    "TODO:  \n",
    "- revise code by finding special symbols that make UnicodeDecodeError to appear. \n",
    "- are there any other less time-consuming way to read files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the amount of dropped e-mails?  - 11877\nWhat is the current amount of e-mails in our dataset?  - 63542\nHow many spam e-mails?  - 40450 \nHow many non-spam e-mails?  - 23092 \n"
     ]
    }
   ],
   "source": [
    "emails = []\n",
    "i = 1\n",
    "count = 0\n",
    "index = -1\n",
    "indexL = []\n",
    "\n",
    "\n",
    "for f in target.file:\n",
    "    index = index + 1\n",
    "    fileLocation = './trec07Data/' + f[3:]\n",
    "    try:\n",
    "        email = open(fileLocation, 'r', encoding='utf8')\n",
    "        emailContent = email.read()\n",
    "        emails.append(emailContent)\n",
    "        email.close()\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "        count = count + 1\n",
    "        indexL.append(index)\n",
    "\n",
    "target.drop(target.index[indexL], inplace=True)\n",
    "\n",
    "print(\"What is the amount of dropped e-mails?  - %i\"%count)\n",
    "print(\"What is the current amount of e-mails in our dataset?  - %i\"% len(emails))\n",
    "\n",
    "printSpamHamStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Third \n",
    "\n",
    "With the help of __email__ library I read a content from each of e-mails while omitting e-mails with 1 character. \n",
    "\n",
    "TODO: Are there any other less time-consuming way to read content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many e-mails are omitted? - 14186 \nHow many e-mails are in the data set? - 49356 \nHow many spam e-mails?  - 28153 \nHow many non-spam e-mails?  - 21203 \n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "\n",
    "allContent = []\n",
    "i = 0\n",
    "index = -1\n",
    "indexL=[]\n",
    "for em in emails:\n",
    "    i+=1\n",
    "    index+=1\n",
    "    msg_out = email.message_from_string(em)\n",
    "    content=''\n",
    "    for part in msg_out.walk():\n",
    "        #part.get_content_type() == 'text/html')or\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            content=content+part.get_payload()\n",
    "    if(len(content)<2):\n",
    "        indexL.append(index)\n",
    "        content=''\n",
    "    else:\n",
    "        allContent.extend([content])\n",
    "\n",
    "target.drop(target.index[indexL], inplace=True)\n",
    "\n",
    "print(\"How many e-mails are omitted? - %i \"%len(indexL))\n",
    "\n",
    "data = pd.DataFrame(pd.Series(allContent), columns=['content'])\n",
    "\n",
    "print(\"How many e-mails are in the data set? - %i \"%data.shape[0])\n",
    "\n",
    "printSpamHamStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Forth\n",
    "\n",
    "Now I assign 1 or 0 instead of 'ham' or 'spam' in __target__ file while creating a dictionary. Indices in __target__ need to be rewritten (0:len(target)) for the further work with newly created __data__ object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = dict(zip(['spam','ham', 'Spam', 'Ham'], [0,1, 0,1]))\n",
    "\n",
    "target = target.set_index(keys = data.index.values)\n",
    "\n",
    "target.replace({'class':map}, inplace=True)\n",
    "\n",
    "data = data.assign(hamOrSpam = target['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>hamOrSpam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi, i've just updated from the gulus and I che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mega  authenticV I A G R A   $ DISCOUNT priceC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nHey Billy, \\n\\nit was really fun going out t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nsystem\" of the home.  It will have the capab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nthe program and the creative abilities of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HoodiaLife - Start Losing Weight Now!\\n\\nHoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nHi...\\n\\nI have to use R to find out the 90%...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Good day!Visit our new online drug store and s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nAnatrim =96 The latest and most delighting p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nmovement on the tablet.  I could even select...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is HGH Life?\\n\\n HGH Life is our patente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hey Billy, \\n\\nit was really fun going out the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>hamOrSpam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi, i've just updated from the gulus and I che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mega  authenticV I A G R A   $ DISCOUNT priceC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nHey Billy, \\n\\nit was really fun going out t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nsystem\" of the home.  It will have the capab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nthe program and the creative abilities of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HoodiaLife - Start Losing Weight Now!\\n\\nHoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nHi...\\n\\nI have to use R to find out the 90%...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Good day!Visit our new online drug store and s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nAnatrim =96 The latest and most delighting p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nmovement on the tablet.  I could even select...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is HGH Life?\\n\\n HGH Life is our patente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hey Billy, \\n\\nit was really fun going out the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fifth\n",
    "\n",
    "Now I can prepare data to feed different ML models. For this purpose I split data into __data_train__ and __data_test__ as well as target data into __y_train__ and __y_test__. Models are learning based on __data_train__ and __y_train__ and predict __y_test__ based trained models and __data_test__. We use *random_state=11* to be able to reproduce the same splitting since we define the seed for the random number generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many samples are included into train data set? - 39484\nHow many samples are included into test data set? - 9872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(data['content'], data['hamOrSpam'], test_size=0.2, \n",
    "                                                          random_state=11)\n",
    "\n",
    "print(\"How many samples are included into train data set? - %i\"%data_train.shape[0])\n",
    "print(\"How many samples are included into test data set? - %i\"%data_test.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sixth\n",
    "\n",
    "Now we check what is the distribution of classes counting amount of spam and non-spam e-mails. In case we have extremely imbalanced classes (10 of 100 are in the minor class) we need to consider better strategies to separate into classes or generate addtional samples of the monir class or reduce a number of samples of the major class, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22513, 16971])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(np.array(y_train).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Seventh\n",
    "\n",
    "Now I prepare text data to feed a model. For this purpose we need train and test data as lists. In the first phase I use __CountVectorizer__ that count a number of words' appearance. This method includes as well ability to ignore *stop_words* that appear often in texts and do not have impact on semantics (e.g. like, the, a, and, or...)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39484, 476006)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_train = data_train.tolist()\n",
    "#y_train = y_train.tolist()\n",
    "#data_test = data_test.tolist()\n",
    "#y_test = y_test.tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english', analyzer='word',\n",
    "                             tokenizer=lambda doc: \n",
    "                                doc.lower().replace('\\n', ' ').\n",
    "                                replace('\\t', ' ').split(' '), lowercase=False)\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(data_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Eight## \n",
    "\n",
    "Let us check what is inside the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('50mg', 105489)\n('$141.02', 16797)\n('$2.95', 17057)\n('$76.68', 18056)\n('$55', 17792)\n('$6', 17858)\n('$1', 16480)\n('day', 197607)\n('term!!!', 432338)\n(\"masters'\", 316865)\n('life', 305666)\n('assured', 148194)\n('distribution.', 206806)\n('rife', 385009)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for k in count_vect.vocabulary_.items():\n",
    "    if i%10==0:\n",
    "        print(k)\n",
    "    i+=1\n",
    "    if i==150:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ninth##\n",
    "\n",
    "Now I transform all counts of words for e-mail to inverse document frequency that emphasize the importance of a word in a collection of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39484, 476006)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tfidf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tfidf.shape  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tenth##\n",
    "\n",
    "Now I can try to use different models with data got from __TfidfTransformer__ where each e-mail is presented as a row with *invese document frequency* in columns that state for words presented in the e-mail and with zeros in other columns. The whole training set of e-mails is then presented as a matrix where columns are words in the training set and vocabulary and rows are training set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\n0.99290923825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "predictedValues = []\n",
    "maxMean = 0\n",
    "bestClass = None\n",
    "for clf in [MultinomialNB(),LogisticRegression(),\n",
    "            SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-5, random_state= 42, max_iter = 5, \n",
    "                                                                tol=None), LinearSVC()]:\n",
    "    clf = clf.fit(X_train_tfidf, y_train)\n",
    "    data_test_counts = count_vect.transform(data_test)\n",
    "    data_test_tfidf = tf_transformer.transform(data_test_counts)\n",
    "    \n",
    "    predicted = clf.predict(data_test_tfidf)\n",
    " \n",
    "    currentMean = np.mean(predicted==np.array(y_test))\n",
    "    if(currentMean>maxMean):\n",
    "        maxMean = currentMean\n",
    "        bestClass = clf\n",
    "        predictedValues = predicted\n",
    "        \n",
    "        \n",
    "print(bestClass)\n",
    "print(maxMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Eleventh##\n",
    "\n",
    "In the code below I trained four models that are commonly used for text classification. In this case __LinearSVC__ is the winner with best accuracy that was calculated just by comparing predicted array with 0s and 1s for each e-mail in a test set with 0s and 1s from an array of actual class of e-mails in the test set (y_test). \n",
    "\n",
    "__LinearSVC__ is Linear Support Vector Classification that is the special case for Support Vector Machines where classification is done using a linear function with no kernels needed (no transformations and non-linear functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Finally##\n",
    "I just want to see a more concrete information about results of the winner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n       spam       0.99      1.00      0.99      5640\n        ham       1.00      0.99      0.99      4232\n\navg / total       0.99      0.99      0.99      9872\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predictedValues, target_names = ['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
